{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3bd6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Memory buffer for on-policy rollouts\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.logps = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def store(self, obs, action, logp, value, reward, done):\n",
    "        self.obs.append(obs.copy())\n",
    "        self.actions.append(action.copy())\n",
    "        self.logps.append(logp)\n",
    "        self.values.append(value)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68edd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d1 = layers.Dense(256, activation='relu')\n",
    "        self.n1 = layers.LayerNormalization()\n",
    "        self.d2 = layers.Dense(256, activation='relu')\n",
    "        self.n2 = layers.LayerNormalization()\n",
    "        self.d3 = layers.Dense(128, activation='tanh')\n",
    "        self.mu = layers.Dense(act_dim)\n",
    "        \n",
    "        self.log_std = self.add_weight(\n",
    "            name='log_std',\n",
    "            shape=(act_dim,),\n",
    "            initializer=tf.constant_initializer(-0.5),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, input):\n",
    "        x = self.d1(input)\n",
    "        x = self.n1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.n2(x)\n",
    "        x = self.d3(x)\n",
    "        mu = self.mu(x)\n",
    "        \n",
    "        batch_size = tf.shape(input)[0]\n",
    "        log_std = tf.broadcast_to(self.log_std[None, :], (batch_size, self.log_std.shape[0]))\n",
    "        return mu, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8904faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\venvs\\main\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\venvs\\main\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\venvs\\main\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, obs_dim, act_dim, \n",
    "                 clip_ratio=0.2, gamma=0.99, lam=0.95,\n",
    "                 pi_lr=1e-4, vf_lr=1e-4, train_pi_iters=80, train_v_iters=80, action_scale=0.9, smooth_coef=0.9):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.train_pi_iters = train_pi_iters\n",
    "        self.train_v_iters = train_v_iters\n",
    "        self.action_scale = action_scale\n",
    "        self.smooth_coef = smooth_coef\n",
    "        self.prev_action = np.zeros(act_dim, dtype=np.float32)\n",
    "\n",
    "        # Build actor and critic\n",
    "        self.actor = Actor(self.obs_dim, self.act_dim)\n",
    "        self.critic = self._build_critic()\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=pi_lr)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=vf_lr)\n",
    "        self.memory = Memory()\n",
    "\n",
    "    def _build_critic(self):\n",
    "        inp = layers.Input(shape=(self.obs_dim,))\n",
    "        x = layers.Dense(256, activation='relu')(inp)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.Dense(128, activation='tanh')(x)\n",
    "        v = layers.Dense(1)(x)\n",
    "        return Model(inputs=inp, outputs=v)\n",
    "\n",
    "    def act(self, obs):\n",
    "        tfd = tfp.distributions\n",
    "        obs = obs.reshape(1, -1).astype(np.float32)\n",
    "        mu, log_std = self.actor(obs)\n",
    "        std = tf.exp(log_std)\n",
    "        pi_dist = tfd.Normal(loc=mu, scale=std)\n",
    "        self.pi_dist = pi_dist\n",
    "        pi = pi_dist.sample()\n",
    "        logp = tf.reduce_sum(pi_dist.log_prob(pi), axis=-1)\n",
    "        value = self.critic(obs)\n",
    "        \n",
    "        raw_action = pi.numpy()[0]\n",
    "\n",
    "        # 4) 액션 스케일링 (크기를 줄인다)\n",
    "        scaled = raw_action * self.action_scale\n",
    "\n",
    "        # 5) 지수 이동 평균 스무딩\n",
    "        smoothed = (self.smooth_coef * self.prev_action\n",
    "                    + (1 - self.smooth_coef) * scaled)\n",
    "        self.prev_action = smoothed\n",
    "\n",
    "        # 6) 환경 허용 범위로 클리핑\n",
    "        action = np.clip(smoothed, -1, 1)\n",
    "\n",
    "        return action, logp.numpy()[0], value.numpy()[0,0]\n",
    "\n",
    "    def store(self, obs, action, logp, value, reward, done):\n",
    "        self.memory.store(obs, action, logp, value, reward, done)\n",
    "\n",
    "    def compute_gae(self, last_value):\n",
    "        rewards = np.array(self.memory.rewards + [0], dtype=np.float32)\n",
    "        values = np.array(self.memory.values + [last_value], dtype=np.float32)\n",
    "        dones = np.array(self.memory.dones + [0], dtype=np.float32)\n",
    "        T = len(self.memory.rewards)\n",
    "        adv = np.zeros(T, dtype=np.float32)\n",
    "        last_gae = 0\n",
    "        for t in reversed(range(T)):\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * (1-dones[t]) - values[t]\n",
    "            adv[t] = last_gae = delta + self.gamma * self.lam * (1-dones[t]) * last_gae\n",
    "        returns = adv + values[:-1]\n",
    "        return adv, returns\n",
    "\n",
    "    def update(self):\n",
    "        obs = np.array(self.memory.obs, dtype=np.float32)\n",
    "        actions = np.array(self.memory.actions, dtype=np.float32)\n",
    "        old_logps = np.array(self.memory.logps, dtype=np.float32)\n",
    "        values = np.array(self.memory.values, dtype=np.float32)\n",
    "        last_value = values[-1]\n",
    "        adv, returns = self.compute_gae(last_value)\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        # Policy update\n",
    "        for _ in range(self.train_pi_iters):\n",
    "            with tf.GradientTape() as tape:\n",
    "                mu, log_std = self.actor(obs)\n",
    "                std = tf.exp(log_std)\n",
    "                new_logp = -0.5 * tf.reduce_sum(((actions - mu)/std)**2 + 2*log_std + np.log(2*np.pi), axis=1)\n",
    "                ratio = tf.exp(new_logp - old_logps)\n",
    "                clipped_ratio = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "                pi_loss = -tf.reduce_mean(tf.minimum(ratio * adv, clipped_ratio * adv))\n",
    "                entropy = tf.reduce_mean(self.pi_dist.entropy())\n",
    "                pi_loss -= 0.01 * entropy\n",
    "\n",
    "            grads = tape.gradient(pi_loss, self.actor.trainable_variables)\n",
    "            self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "\n",
    "        # Value update\n",
    "        for _ in range(self.train_v_iters):\n",
    "            with tf.GradientTape() as tape:\n",
    "                v = self.critic(obs)[:,0]\n",
    "                v_loss = tf.reduce_mean((returns - v)**2)\n",
    "            grads = tape.gradient(v_loss, self.critic.trainable_variables)\n",
    "            self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b64c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Return 158.9, Length 1000\n"
     ]
    }
   ],
   "source": [
    "def train_ppo(env_name=\"Ant-v5\", epochs=1, steps_per_epoch=1000):\n",
    "    env = gym.make(env_name, render_mode='human')\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    agent = PPOAgent(obs_dim, act_dim)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        obs, info = env.reset()\n",
    "        ep_ret, ep_len = 0, 0\n",
    "        for t in range(steps_per_epoch):\n",
    "            action, logp, value = agent.act(obs)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.store(obs, action, logp, value, reward, done)\n",
    "            obs = next_obs\n",
    "            ep_ret += reward\n",
    "            ep_len += 1\n",
    "\n",
    "            if epoch+1 == epochs and (t==steps_per_epoch-1) or done:\n",
    "                _, _, last_val = agent.act(obs)\n",
    "                agent.memory.values[-1] = last_val\n",
    "            \n",
    "                last_val = agent.memory.values[-1]\n",
    "                adv, returns = agent.compute_gae(last_val)\n",
    "                      \n",
    "                print(f\"Epoch {epoch}: Return {ep_ret:.1f}, Length {ep_len}\")\n",
    "                obs, info = env.reset()\n",
    "                ep_ret, ep_len = 0, 0\n",
    "                \n",
    "                agent.update()\n",
    "            elif done or (t==steps_per_epoch-1):\n",
    "                _, _, last_val = agent.act(obs)\n",
    "                agent.memory.values[-1] = last_val\n",
    "                agent.update()\n",
    "                                \n",
    "                print(f\"Epoch {epoch}: Return {ep_ret:.1f}, Length {ep_len}\")\n",
    "                obs, info = env.reset()\n",
    "                ep_ret, ep_len = 0, 0\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    actor_model = agent.actor\n",
    "    actor_model.save(\"actor_model_PPO.keras\")\n",
    "    env.close()\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    train_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f41f5b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmp4eft_lak\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmp4eft_lak\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmp4eft_lak'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 105), dtype=tf.float32, name='input_layer_4')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(1, 8), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2771178222032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771178219344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246268240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246260560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246267472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246258832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246254608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246257104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpa9ucjl2v\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpa9ucjl2v\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpa9ucjl2v'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 105), dtype=tf.float32, name='input_layer_4')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(1, 8), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2771178222032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771178219344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246268240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246260560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246267472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246258832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246254608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2771246257104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 저장된 모델 불러오기\n",
    "model = tf.keras.models.load_model(\"actor_model.keras\")\n",
    "\n",
    "# 2. 기본 TFLite 변환 (float32 모델)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 3. 저장\n",
    "with open(\"actor_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# 4. 동적 범위 양자화 적용\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)  # 다시 converter 초기화\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# 5. 양자화된 모델 저장\n",
    "with open(\"actor_model_quant_dynamic.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27a6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"actor_model_quant_dynamic.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "env = gym.make('Ant-v5', render_mode='human')\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    input_data = np.array(obs, dtype=np.float32).reshape(1, -1)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    action = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "    \n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "a = input()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342da43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
