{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ec7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, act_dim):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.obs_buf = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros((max_size, 1), np.float32)\n",
    "        self.done_buf = np.zeros((max_size, 1), np.float32)\n",
    "        \n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.max_size, size=batch_size)\n",
    "        \n",
    "        return dict(obs = self.obs_buf[idxs],\n",
    "                    act = self.act_buf[idxs],\n",
    "                    rews = self.rew_buf[idxs],\n",
    "                    next_obs = self.next_obs_buf[idxs],\n",
    "                    done = self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6befdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ActorCritic(keras.Model):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = keras.Sequential([\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.LayerNormalization(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(act_dim, activation='tanh')\n",
    "        ])\n",
    "        \n",
    "        self.critic = keras.Sequential([\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.LayerNormalization(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        obs = inputs\n",
    "        return self.actor(obs), self.critic(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639a05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.model = ActorCritic(obs_dim, act_dim)\n",
    "        self.target_model = ActorCritic(obs_dim, act_dim)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.buffer = ReplayBuffer(max_size=100000, obs_dim=obs_dim, act_dim=act_dim)\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "        \n",
    "    def act(self, obs):\n",
    "        obs = tf.convert_to_tensor(obs.reshape(1, -1), dtype=np.float32)\n",
    "        action, _ = self.model(obs)\n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.batch_size > self.buffer.size:\n",
    "            return\n",
    "        \n",
    "        batch = self.buffer.sample_batch(self.batch_size)\n",
    "        \n",
    "        obs = tf.convert_to_tensor(batch['obs'], dtype=np.float32)\n",
    "        act = tf.convert_to_tensor(batch['act'], dtype=np.float32)\n",
    "        rews = tf.convert_to_tensor(batch['rews'], dtype=np.float32)\n",
    "        next_obs = tf.convert_to_tensor(batch['next_obs'], dtype=np.float32)\n",
    "        done = tf.convert_to_tensor(batch['done'], dtype=np.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            _, value = self.model(obs)\n",
    "            _, next_value = self.target_model(next_obs)\n",
    "            target = rews + self.gamma*(1-done)*next_value\n",
    "            critic_loss = tf.reduce_mean((value - target)**2)\n",
    "            \n",
    "        gradient = tape.gradient(critic_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "        \n",
    "        tau = 0.005\n",
    "        new_weights = []\n",
    "        target_variables = self.target_model.weights\n",
    "        for i, variable in enumerate(self.model.weights):\n",
    "            new_weights.append(tau * variable + (1 - tau) * target_variables[i])\n",
    "        self.target_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a416ab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total reward: -35.18639724309523\n",
      "episode: 1, total reward: -77.57735372755681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venvs\\main\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['actor_critic_2/sequential_4/dense_12/kernel', 'actor_critic_2/sequential_4/dense_12/bias', 'actor_critic_2/sequential_4/layer_normalization_4/gamma', 'actor_critic_2/sequential_4/layer_normalization_4/beta', 'actor_critic_2/sequential_4/dense_13/kernel', 'actor_critic_2/sequential_4/dense_13/bias', 'actor_critic_2/sequential_4/dense_14/kernel', 'actor_critic_2/sequential_4/dense_14/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2, total reward: -148.10883900830652\n",
      "episode: 3, total reward: -237.88043305341202\n",
      "episode: 4, total reward: -2095.3317022042615\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "rew_step = []\n",
    "\n",
    "env = gym.make('Ant-v5', render_mode='human')\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "agent = Agent(obs_dim, act_dim)\n",
    "\n",
    "num_episodes = 5\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    waiting_time = 0\n",
    "    \n",
    "    for t in range(1000): # 5000frame --> step*5 = frame\n",
    "        action = agent.act(obs)\n",
    "        noise_scale = max(0.1, 1.0 - ep / num_episodes)  # 점점 줄이기\n",
    "        noise = np.random.normal(0, noise_scale, size=action.shape)\n",
    "        action += noise\n",
    "\n",
    "        action = np.clip(action, -1, 1)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        shaped_reward = reward\n",
    "        \n",
    "        foward_progress = abs(next_obs[0] - obs[0])\n",
    "        shaped_reward += foward_progress*5\n",
    "        \n",
    "        pre_z_posision = obs[2]\n",
    "        z_position = next_obs[2]\n",
    "        waiting_penalty = 0\n",
    "        if abs(pre_z_posision-z_position) + abs(next_obs[0]-obs[0]) < 0.05:\n",
    "            waiting_time += 1\n",
    "        else:\n",
    "            waiting_time = 0\n",
    "            \n",
    "        if waiting_time > 20:\n",
    "            waiting_time = 0\n",
    "            waiting_penalty = 20\n",
    "            \n",
    "        shaped_reward -= waiting_penalty\n",
    "        waiting_penalty = 0\n",
    "        \n",
    "        if z_position < 0.3:\n",
    "            penalty = 1\n",
    "        else:\n",
    "            penalty = 0\n",
    "        shaped_reward -= penalty\n",
    "        \n",
    "        agent.buffer.store(obs, action, shaped_reward, next_obs, done)\n",
    "        agent.learn()\n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += shaped_reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # Actor만 따로 저장\n",
    "    actor_model = agent.model.actor\n",
    "    actor_model.save(\"actor_model.keras\")\n",
    "\n",
    "        \n",
    "    print(f\"episode: {ep}, total reward: {total_reward}\")\n",
    "    rew_step.append(total_reward)\n",
    "\n",
    "input()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7eba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpl9nqh7ba\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpl9nqh7ba\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpl9nqh7ba'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 105), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(1, 8), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1787755393488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787755381584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363675792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363684816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363684624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363680976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363673680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363681936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpyuvbnxnq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpyuvbnxnq\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\dlwns\\AppData\\Local\\Temp\\tmpyuvbnxnq'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 105), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(1, 8), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1787755393488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787755381584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363675792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363684816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363684624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363680976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363673680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1787363681936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 저장된 모델 불러오기\n",
    "model = tf.keras.models.load_model(\"actor_model.keras\")\n",
    "\n",
    "# 2. 기본 TFLite 변환 (float32 모델)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 3. 저장\n",
    "with open(\"actor_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# 4. 동적 범위 양자화 적용\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)  # 다시 converter 초기화\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# 5. 양자화된 모델 저장\n",
    "with open(\"actor_model_quant_dynamic.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5210205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"actor_model_quant_dynamic.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "env = gym.make('Ant-v5', render_mode='human')\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    input_data = np.array(obs, dtype=np.float32).reshape(1, -1)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    action = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "    \n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "a = input()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3ea14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
